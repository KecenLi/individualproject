# Meeting Follow‑Up Report (Post‑Meeting Work Summary)

这次会后工作重点是把 **OOD coverage（NAC‑UE）官方流程**完整跑通、将 **APS 与非 APS 的差异**用可复现的证据说明，并用**定性检验**补充解释力，最终形成可以直接交付导师的图像与结论。整体思路是：先确保官方流程无偏执行，再用“ID‑only 阈值”与“Top/Bottom 样本可视化”来验证方法是否依赖 OOD 校准、以及 NAC 是否真的反映内部可靠性。

第一部分是官方流程本体。我们以 CIFAR‑10 为基准，按 OpenOOD v1.5 的官方评测脚本完成 NAC‑UE 推理与评测输出。结论层面，不做 OOD 校准（非 APS）时，整体表现仍保持稳定：Far‑OOD 综合 AUROC 约 **94**、FPR@95 约 **19**，说明 NAC 的核心行为并未依赖 OOD 先验；而 APS 模式在 Near‑OOD 上并不提供真实指标（官方设为占位值），因此在讨论校准依赖性时，必须以非 APS 的结果为主。为方便导师查验，我保存了官方输出与分析文件，并在后续统一整理。

第二部分是 APS vs 非 APS 的比较。我们用同一模型与相同评测数据做对照，目的是明确“APS 作为上界、非 APS 作为现实场景”的定位。结论是 APS 应被视为理想化上界（使用 OOD 验证集调参），而非 APS 更接近“未知 OOD”场景。这个对比直接回应了“过拟合/校准依赖”的疑虑：如果只看 APS，可能高估方法实际可用性；若看非 APS，能观察到更真实的分离能力（例如 Near‑OOD 在非 APS 下 AUROC 约 **91**、FPR@95 约 **30**）。比较结果已经整理成一张清晰对照表与图片，便于导师快速阅读。

![APS vs Non‑APS 对比](ood_coverage/analysis/nac_aps_vs_nonaps.png)

第三部分是 ID 与 OOD 的 Top/Bottom 质检。这一步不是为了追求数值，而是为了验证 NAC 的“语义一致性”：高分样本是否更稳定、低分样本是否更难。ID 方面的可视化显示，结构明确、姿态稳定的类别更容易获得高 NAC，纹理复杂或姿态变化大的类别更容易落在低 NAC 区间，且低 NAC 与误分类风险存在明显相关。OOD 方面则分别在 APS 与非 APS 两种设置下做了 Top/Bottom 检查，观察到不同 OOD 类型的高/低分样本存在可见的风格差异，提示 NAC 确实在捕捉分布特征，但不同 OOD 类型的可分性差异明显。

![ID Top/Bottom 示例](ood_coverage/analysis/nac_top_bottom_20260127_130431/top100_grid.png)

第四部分是“ID‑only 阈值”的可行性验证。为了回应“没有 OOD 校准是否还能用”的问题，我从 ID 验证集出发构建固定阈值（1%/5% 分位）并评估 OOD 检出率，同时对照传统的 FPR@95 阈值。结论上，ID‑only 阈值在 Far‑OOD 上仍具一定检测能力，但在 Near‑OOD 上明显不足：以 **1% ID‑FPR** 为阈值时，Near‑OOD 的检出率大致只有 **11–15%**；即使放宽到 **5% ID‑FPR**，Near‑OOD 也仅到 **45–51%**。这表明 NAC 在缺少 OOD 校准时并非不可用，但可用性更依赖 OOD 类型，且存在明确的性能下限。该分析已整理为对比表与图，可以直接放入报告。

![ID‑only 阈值对比](ood_coverage/analysis/nac_idonly_thresholds_20260127_140612/threshold_table.png)

总体结论是：我们已经用官方流程完成了 NAC‑UE 的“可运行、可复核、可解释”闭环，并给出了三条对导师问题最直接的回答：1）APS 是上界，非 APS 才是现实场景；2）NAC 的稳定性与样本结构/类别特征相关，具备可解释性；3）ID‑only 阈值可以作为无需 OOD 的保守基线，但对 Near‑OOD 效果有限。这些内容已整理为一页总结图和若干可视化材料，便于导师快速评阅。

![一页总结](ood_coverage/analysis/nac_onepage_summary_20260127/nac_onepage_summary.png)

---

# 非技术版详细说明（面向不了解细节的督导）

下面这部分用更直观的方式解释我完成了什么、为什么重要、每一步解决了什么问题。**只在 NAC 的概念上点到为止，其他内容尽量展开说明**。

## 1. 这项工作的“问题背景”
我们的任务不是“训练更准的模型”，而是回答一个更现实的问题：**当模型遇到“不同于训练数据”的输入时，我们能否用内部信号判断它是否可靠？**  
这里的“不同于训练数据”包括：  
- 分布漂移（例如加入噪声、换成另一种数据集）  
- 自然扰动（例如模糊、压缩、形变）  
- 对抗性扰动（攻击者刻意欺骗）

我们采用的检测信号是 **NAC（Neuron Activation Coverage）**，它衡量的是**模型内部神经元是否被充分激活**。  
直观理解：如果模型对输入“看起来熟悉”，它会激活一套稳定的内部通路；如果输入“陌生或异常”，激活模式会发生偏移。  
我们不是要“重新证明 NAC 有效”，而是要**把官方流程跑通，并验证它在现实条件下是否可信**。

## 2. 我完成的第一件事：跑通官方流程（保证“方法正确执行”）
我用 **官方的 OpenOOD v1.5 评测脚本**完整跑了一遍 NAC‑UE。  
这一步非常关键，因为它确保：  
- 所有评测都是按论文与官方代码路径执行  
- 结果具有可复现性，能直接对照论文  
- 为后续分析提供可信基线  

这一步的意义是：**先保证流程正确，再讨论结果好坏**。如果流程不对，所有结论都可能是错的。

## 3. 第二件事：解释 APS vs 非 APS（回答“会不会过拟合/依赖校准”）
论文里有一个关键点：**APS 会用 OOD 数据来调参**。  
这就像考试前提前看了部分题目，会让成绩看起来更高。  
因此我做了 **APS vs 非 APS** 的对比，核心目的不是追求高分，而是验证“**不看 OOD 的情况下还行不行**”。

这一步得到的结论是：  
- APS 是理想化上界（看过 OOD，表现自然更好）  
- 非 APS 才是现实场景（不知道 OOD，会更真实）  
因此我把 APS 当成“上界”，把非 APS 当作“真实可用性”。

![APS vs Non‑APS 对比](ood_coverage/analysis/nac_aps_vs_nonaps.png)

## 4. 第三件事：做 Top/Bottom 质检（回答“分数是否有意义”）
光看指标不够，我做了可视化检查：  
把 NAC 分数最高和最低的样本分别拉出来，看看这些样本“长什么样”。

发现的直观规律是：  
- 高分样本通常结构清晰、姿态稳定  
- 低分样本通常纹理复杂、姿态变化大  
- 低分更容易被模型预测错  

这一步的意义是：  
NAC 的分数不是随机的，它和“样本是否稳定、是否容易分类”是有关系的，因此分数具备可解释性。

![ID Top/Bottom 示例](ood_coverage/analysis/nac_top_bottom_20260127_130431/top100_grid.png)

## 5. 第四件事：只用 ID 数据设阈值（回答“没 OOD 能不能用”）
在现实里，我们往往拿不到 OOD 数据来调参，所以我做了一个更保守的实验：  
只用 ID 数据（训练/验证集）来设定阈值，再看能不能检测 OOD。

结果说明：  
- 对远域 OOD，NAC 仍有一定检出能力  
- 对近域 OOD，效果明显变弱  
这意味着：**NAC 不是完全依赖 OOD 校准，但在近域情况下会明显受限**。  
这是对论文“高分数表现”的一个现实补充。

![ID‑only 阈值对比](ood_coverage/analysis/nac_idonly_thresholds_20260127_140612/threshold_table.png)

## 6. 这说明了什么（总结给督导）
如果用一句话总结我做的事：  
**我把官方 NAC‑UE 流程跑通，并验证了它在“现实条件”下的可靠程度，同时提供了可解释的质检证据。**

更具体的三点结论：  
1. 官方流程可复现，结果可信，不是自造流程  
2. APS 是上界，非 APS 是现实：没有 OOD 校准时，表现明显下降，尤其是近域 OOD  
3. NAC 分数有语义意义：Top/Bottom 样本显示内部信号确实反映样本稳定性  

这份结论既回应了“会不会过拟合/依赖 OOD”的疑问，也为下一步对抗扰动或多攻击比较提供了可信基线。
